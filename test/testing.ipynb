{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('default_of_credit_card_clients.csv')\n",
    "df.drop(columns=['ID'],inplace=True)\n",
    "target = 'default_payment_next_month'\n",
    "target_rename = 'default'\n",
    "df.rename(columns={target:target_rename}, inplace=True)\n",
    "target=target_rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_standardize(df=None,target=None):\n",
    "    df_res = df.copy()\n",
    "    for col in df_res.columns[df_res.columns != target]:\n",
    "        df_res[col] = (df_res[col] - df_res[col].mean()) / df_res[col].std()\n",
    "    return df_res\n",
    "\n",
    "df_std = data_standardize(df, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df=None, label=None, validation=False, train_size=0.8, random_state=42, tensor=False):\n",
    "    if validation == False and tensor == False:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df.iloc[:,df.columns != label], df.iloc[:,df.columns == label], \n",
    "                                                            test_size=(1-train_size), random_state=random_state)\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    elif validation == True and tensor == True:\n",
    "        x_train, x_val_te, y_train, y_val_te = train_test_split(df.iloc[:,df.columns != label], df.iloc[:,df.columns == label], \n",
    "                                                            test_size=(1-train_size), random_state=random_state)\n",
    "        x_val, x_test, y_val, y_test = train_test_split(x_val_te, y_val_te, \n",
    "                                                            test_size=0.5, random_state=random_state)\n",
    "        X_train = torch.Tensor(x_train.values)\n",
    "        X_val = torch.Tensor(x_val.values)\n",
    "        X_test = torch.Tensor(x_test.values)\n",
    "        Y_train = torch.Tensor(y_train.values)\n",
    "        Y_val = torch.Tensor(y_val.values)\n",
    "        Y_test = torch.Tensor(y_test.values)\n",
    "        return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
    "\n",
    "x_train_nn, x_val_nn, x_test_nn, y_train_nn, y_val_nn, y_test_nn = data_split(df=df_std, label=target\n",
    "                                                                                  ,validation=True, train_size=0.8\n",
    "                                                                                  ,tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(x_train_nn, y_train_nn), batch_size=nn_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(x_val_nn, y_val_nn), batch_size=nn_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(x_test_nn, y_test_nn), batch_size=nn_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_nn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_feature, dim1, dim2, drop, output_feature):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.Linear(input_feature, dim1, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.model2 = nn.Sequential(\n",
    "            nn.Linear(dim1, dim2, bias=True),\n",
    "            nn.Dropout(drop),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.model3 = nn.Sequential(\n",
    "            nn.Linear(dim2, output_feature, bias=True),\n",
    "        )   \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x1 = self.model1(input)\n",
    "        x2 = self.model2(x1)\n",
    "        output = self.model3(x2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def training_nn(epochs=500, learning_rate=0.01, loss_func='CE', optimizer_para='Adam',\n",
    "                input_feature=23, output_feature=2, dim1=36, dim2=108, dropout=0.8, val_size = None,\n",
    "                train_loader=None, val_loader=None, test_loader=None, device=None):\n",
    "    # Initialize model\n",
    "    mynetwork = MyNetwork(input_feature=input_feature, dim1=dim1, dim2=dim2, drop=dropout, output_feature=output_feature)\n",
    "    mynetwork.to(device)\n",
    "\n",
    "    # Define loss function & optimizer\n",
    "    if loss_func == 'CE':\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss_fn.to(device)\n",
    "    if optimizer_para == 'Adam':\n",
    "        optimizer = optim.SGD(mynetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Epoches, training & validation\n",
    "    total_training_step, total_val_step = 0, 0\n",
    "    writer = SummaryWriter(\"../logs_train\")\n",
    "    for i in range(epochs):\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f\"------------epoch: {i+1}------------\")\n",
    "        \"\"\"\n",
    "        Training\n",
    "        \"\"\"\n",
    "        mynetwork.train()\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.view(-1, input_feature)\n",
    "            labels = labels.squeeze().long()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = mynetwork(inputs)\n",
    "            loss = loss_fn(outputs, labels.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_training_step += 1\n",
    "            if total_training_step % 100 == 0:\n",
    "                print(f\"Traning: {total_training_step}; Loss: {loss.item()}\")\n",
    "                writer.add_scalar(\"train_loss\", loss.item(), total_training_step)\n",
    "\n",
    "        \"\"\"\n",
    "        Validation\n",
    "        \"\"\"\n",
    "        mynetwork.eval()\n",
    "        total_test_loss, total_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.view(-1, input_feature)\n",
    "                labels = labels.squeeze().long()\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = mynetwork(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_test_loss += loss.item()\n",
    "                acc = (outputs.argmax(1) == labels).sum()\n",
    "                total_acc += acc\n",
    "        print(f\"total test loss: {total_test_loss}\")\n",
    "        print(f\"total accuracy: {total_acc/val_size}\")\n",
    "        writer.add_scalar(\"test_loss\", total_test_loss, total_val_step)\n",
    "        writer.add_scalar(\"test_accuracy\", total_acc/val_size, total_val_step)\n",
    "        total_val_step += 1\n",
    "\n",
    "        torch.save(mynetwork, f\"mynetwork_{i}.pth\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning: 100; Loss: 0.49957379698753357\n",
      "Traning: 200; Loss: 0.46580809354782104\n",
      "Traning: 300; Loss: 0.5168883800506592\n",
      "total test loss: 23.744441151618958\n",
      "total accuracy: 0.7836666703224182\n",
      "Traning: 400; Loss: 0.5825622081756592\n",
      "Traning: 500; Loss: 0.5340575575828552\n",
      "Traning: 600; Loss: 0.44454991817474365\n",
      "Traning: 700; Loss: 0.399914413690567\n",
      "total test loss: 22.917233496904373\n",
      "total accuracy: 0.7889999747276306\n",
      "Traning: 800; Loss: 0.5146798491477966\n",
      "Traning: 900; Loss: 0.5637843012809753\n",
      "Traning: 1000; Loss: 0.3446252942085266\n",
      "Traning: 1100; Loss: 0.5287122130393982\n",
      "total test loss: 22.39302009344101\n",
      "total accuracy: 0.793666660785675\n",
      "Traning: 1200; Loss: 0.4272773265838623\n",
      "Traning: 1300; Loss: 0.4492266774177551\n",
      "Traning: 1400; Loss: 0.42827507853507996\n",
      "Traning: 1500; Loss: 0.4130016565322876\n",
      "total test loss: 22.066223055124283\n",
      "total accuracy: 0.8013333082199097\n",
      "Traning: 1600; Loss: 0.5021511316299438\n",
      "Traning: 1700; Loss: 0.47557103633880615\n",
      "Traning: 1800; Loss: 0.5340029001235962\n",
      "total test loss: 21.856694400310516\n",
      "total accuracy: 0.8050000071525574\n",
      "Traning: 1900; Loss: 0.3961744010448456\n",
      "Traning: 2000; Loss: 0.6585487127304077\n",
      "Traning: 2100; Loss: 0.5025211572647095\n",
      "Traning: 2200; Loss: 0.38155457377433777\n",
      "total test loss: 21.71527597308159\n",
      "total accuracy: 0.8050000071525574\n",
      "Traning: 2300; Loss: 0.5188925862312317\n",
      "Traning: 2400; Loss: 0.38487890362739563\n",
      "Traning: 2500; Loss: 0.44338127970695496\n",
      "Traning: 2600; Loss: 0.5231406092643738\n",
      "total test loss: 21.60634931921959\n",
      "total accuracy: 0.8046666383743286\n",
      "Traning: 2700; Loss: 0.3697337508201599\n",
      "Traning: 2800; Loss: 0.32255929708480835\n",
      "Traning: 2900; Loss: 0.45485052466392517\n",
      "Traning: 3000; Loss: 0.4654456079006195\n",
      "total test loss: 21.508763432502747\n",
      "total accuracy: 0.8063333630561829\n",
      "Traning: 3100; Loss: 0.37531137466430664\n",
      "Traning: 3200; Loss: 0.5398778319358826\n",
      "Traning: 3300; Loss: 0.5126381516456604\n",
      "total test loss: 21.45679146051407\n",
      "total accuracy: 0.8059999942779541\n",
      "Traning: 3400; Loss: 0.4748823940753937\n",
      "Traning: 3500; Loss: 0.3965685963630676\n",
      "Traning: 3600; Loss: 0.6456173062324524\n",
      "Traning: 3700; Loss: 0.5541661381721497\n",
      "total test loss: 21.363417387008667\n",
      "total accuracy: 0.8063333630561829\n",
      "Traning: 3800; Loss: 0.3978496491909027\n",
      "Traning: 3900; Loss: 0.4746650755405426\n",
      "Traning: 4000; Loss: 0.38193368911743164\n",
      "Traning: 4100; Loss: 0.4250965714454651\n",
      "total test loss: 21.324864268302917\n",
      "total accuracy: 0.8063333630561829\n",
      "Traning: 4200; Loss: 0.4300880432128906\n",
      "Traning: 4300; Loss: 0.6139028668403625\n",
      "Traning: 4400; Loss: 0.5183321833610535\n",
      "Traning: 4500; Loss: 0.47793304920196533\n",
      "total test loss: 21.254161059856415\n",
      "total accuracy: 0.8080000281333923\n",
      "Traning: 4600; Loss: 0.39832520484924316\n",
      "Traning: 4700; Loss: 0.514897882938385\n",
      "Traning: 4800; Loss: 0.504960834980011\n",
      "total test loss: 21.204496681690216\n",
      "total accuracy: 0.8096666932106018\n",
      "Traning: 4900; Loss: 0.4831138551235199\n",
      "Traning: 5000; Loss: 0.5193802118301392\n",
      "Traning: 5100; Loss: 0.46374836564064026\n",
      "Traning: 5200; Loss: 0.4118306636810303\n",
      "total test loss: 21.195757925510406\n",
      "total accuracy: 0.8076666593551636\n",
      "Traning: 5300; Loss: 0.5285506248474121\n",
      "Traning: 5400; Loss: 0.5323187708854675\n",
      "Traning: 5500; Loss: 0.2977758049964905\n",
      "Traning: 5600; Loss: 0.5247819423675537\n",
      "total test loss: 21.147685289382935\n",
      "total accuracy: 0.8096666932106018\n",
      "Traning: 5700; Loss: 0.49086761474609375\n",
      "Traning: 5800; Loss: 0.5031505823135376\n",
      "Traning: 5900; Loss: 0.559357225894928\n",
      "Traning: 6000; Loss: 0.520115077495575\n",
      "total test loss: 21.131593376398087\n",
      "total accuracy: 0.8116666674613953\n",
      "Traning: 6100; Loss: 0.31973996758461\n",
      "Traning: 6200; Loss: 0.33508142828941345\n",
      "Traning: 6300; Loss: 0.4385966360569\n",
      "total test loss: 21.104907482862473\n",
      "total accuracy: 0.8109999895095825\n",
      "Traning: 6400; Loss: 0.6324525475502014\n",
      "Traning: 6500; Loss: 0.47762811183929443\n",
      "Traning: 6600; Loss: 0.4868753254413605\n",
      "Traning: 6700; Loss: 0.446869820356369\n",
      "total test loss: 21.084906607866287\n",
      "total accuracy: 0.8106666803359985\n",
      "Traning: 6800; Loss: 0.4902220666408539\n",
      "Traning: 6900; Loss: 0.47820332646369934\n",
      "Traning: 7000; Loss: 0.49730440974235535\n",
      "Traning: 7100; Loss: 0.5005511045455933\n",
      "total test loss: 21.095004469156265\n",
      "total accuracy: 0.8116666674613953\n",
      "Traning: 7200; Loss: 0.46493032574653625\n",
      "Traning: 7300; Loss: 0.3994572162628174\n",
      "Traning: 7400; Loss: 0.48934832215309143\n",
      "Traning: 7500; Loss: 0.35480549931526184\n",
      "total test loss: 21.034485429525375\n",
      "total accuracy: 0.8119999766349792\n",
      "Traning: 7600; Loss: 0.435880184173584\n",
      "Traning: 7700; Loss: 0.502088725566864\n",
      "Traning: 7800; Loss: 0.5787854194641113\n",
      "total test loss: 21.02194109559059\n",
      "total accuracy: 0.812666654586792\n",
      "Traning: 7900; Loss: 0.3268675208091736\n",
      "Traning: 8000; Loss: 0.38459599018096924\n",
      "Traning: 8100; Loss: 0.48614639043807983\n",
      "Traning: 8200; Loss: 0.522782564163208\n",
      "total test loss: 20.998502016067505\n",
      "total accuracy: 0.8136666417121887\n",
      "Traning: 8300; Loss: 0.47722330689430237\n",
      "Traning: 8400; Loss: 0.4729921817779541\n",
      "Traning: 8500; Loss: 0.5042744874954224\n",
      "Traning: 8600; Loss: 0.41150298714637756\n",
      "total test loss: 21.000390470027924\n",
      "total accuracy: 0.8130000233650208\n",
      "Traning: 8700; Loss: 0.39692631363868713\n",
      "Traning: 8800; Loss: 0.5742253661155701\n",
      "Traning: 8900; Loss: 0.4297054708003998\n",
      "Traning: 9000; Loss: 0.5542793869972229\n",
      "total test loss: 20.980637222528458\n",
      "total accuracy: 0.812666654586792\n",
      "Traning: 9100; Loss: 0.37499892711639404\n",
      "Traning: 9200; Loss: 0.47716277837753296\n",
      "Traning: 9300; Loss: 0.5907083749771118\n",
      "total test loss: 20.96723958849907\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 9400; Loss: 0.3541741371154785\n",
      "Traning: 9500; Loss: 0.41099780797958374\n",
      "Traning: 9600; Loss: 0.4443110525608063\n",
      "Traning: 9700; Loss: 0.4350402057170868\n",
      "total test loss: 20.95025870203972\n",
      "total accuracy: 0.8133333325386047\n",
      "Traning: 9800; Loss: 0.3999815583229065\n",
      "Traning: 9900; Loss: 0.46615085005760193\n",
      "Traning: 10000; Loss: 0.3770618140697479\n",
      "Traning: 10100; Loss: 0.3319513201713562\n",
      "total test loss: 20.924597173929214\n",
      "total accuracy: 0.8136666417121887\n",
      "Traning: 10200; Loss: 0.5730389952659607\n",
      "Traning: 10300; Loss: 0.4496140480041504\n",
      "Traning: 10400; Loss: 0.47866055369377136\n",
      "Traning: 10500; Loss: 0.38444289565086365\n",
      "total test loss: 20.908132642507553\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 10600; Loss: 0.5301086902618408\n",
      "Traning: 10700; Loss: 0.4440501034259796\n",
      "Traning: 10800; Loss: 0.46960118412971497\n",
      "total test loss: 20.89606711268425\n",
      "total accuracy: 0.8133333325386047\n",
      "Traning: 10900; Loss: 0.3989960849285126\n",
      "Traning: 11000; Loss: 0.3618524968624115\n",
      "Traning: 11100; Loss: 0.5204355716705322\n",
      "Traning: 11200; Loss: 0.43238192796707153\n",
      "total test loss: 20.896879106760025\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 11300; Loss: 0.398592472076416\n",
      "Traning: 11400; Loss: 0.3894989490509033\n",
      "Traning: 11500; Loss: 0.5697486996650696\n",
      "Traning: 11600; Loss: 0.4643184244632721\n",
      "total test loss: 20.884228378534317\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 11700; Loss: 0.44881460070610046\n",
      "Traning: 11800; Loss: 0.4760163724422455\n",
      "Traning: 11900; Loss: 0.5423062443733215\n",
      "Traning: 12000; Loss: 0.5673038959503174\n",
      "total test loss: 20.896171063184738\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 12100; Loss: 0.5219435691833496\n",
      "Traning: 12200; Loss: 0.4309401512145996\n",
      "Traning: 12300; Loss: 0.36396846175193787\n",
      "total test loss: 20.873908817768097\n",
      "total accuracy: 0.8133333325386047\n",
      "Traning: 12400; Loss: 0.4069519639015198\n",
      "Traning: 12500; Loss: 0.38766711950302124\n",
      "Traning: 12600; Loss: 0.3400212228298187\n",
      "Traning: 12700; Loss: 0.3220703601837158\n",
      "total test loss: 20.85952779650688\n",
      "total accuracy: 0.812666654586792\n",
      "Traning: 12800; Loss: 0.4534154236316681\n",
      "Traning: 12900; Loss: 0.4057267904281616\n",
      "Traning: 13000; Loss: 0.46883058547973633\n",
      "Traning: 13100; Loss: 0.4492972791194916\n",
      "total test loss: 20.85746058821678\n",
      "total accuracy: 0.8133333325386047\n",
      "Traning: 13200; Loss: 0.489340215921402\n",
      "Traning: 13300; Loss: 0.4394600987434387\n",
      "Traning: 13400; Loss: 0.4172670245170593\n",
      "Traning: 13500; Loss: 0.3200922906398773\n",
      "total test loss: 20.826764315366745\n",
      "total accuracy: 0.8130000233650208\n",
      "Traning: 13600; Loss: 0.5527278780937195\n",
      "Traning: 13700; Loss: 0.4032979905605316\n",
      "Traning: 13800; Loss: 0.5784018039703369\n",
      "total test loss: 20.83032986521721\n",
      "total accuracy: 0.8136666417121887\n",
      "Traning: 13900; Loss: 0.38867199420928955\n",
      "Traning: 14000; Loss: 0.45959004759788513\n",
      "Traning: 14100; Loss: 0.5201170444488525\n",
      "Traning: 14200; Loss: 0.47402337193489075\n",
      "total test loss: 20.84192591905594\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 14300; Loss: 0.44636404514312744\n",
      "Traning: 14400; Loss: 0.5056473612785339\n",
      "Traning: 14500; Loss: 0.45467573404312134\n",
      "Traning: 14600; Loss: 0.3938625752925873\n",
      "total test loss: 20.81486541032791\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 14700; Loss: 0.3547956645488739\n",
      "Traning: 14800; Loss: 0.4198666512966156\n",
      "Traning: 14900; Loss: 0.5469962954521179\n",
      "Traning: 15000; Loss: 0.5119815468788147\n",
      "total test loss: 20.816977351903915\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 15100; Loss: 0.48210689425468445\n",
      "Traning: 15200; Loss: 0.3521466553211212\n",
      "Traning: 15300; Loss: 0.4048364758491516\n",
      "total test loss: 20.812271058559418\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 15400; Loss: 0.33982858061790466\n",
      "Traning: 15500; Loss: 0.439081609249115\n",
      "Traning: 15600; Loss: 0.4229738414287567\n",
      "Traning: 15700; Loss: 0.45154592394828796\n",
      "total test loss: 20.81005945801735\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 15800; Loss: 0.3938775360584259\n",
      "Traning: 15900; Loss: 0.4193657338619232\n",
      "Traning: 16000; Loss: 0.46147042512893677\n",
      "Traning: 16100; Loss: 0.4688372313976288\n",
      "total test loss: 20.79023775458336\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 16200; Loss: 0.42999276518821716\n",
      "Traning: 16300; Loss: 0.3976908326148987\n",
      "Traning: 16400; Loss: 0.4480617046356201\n",
      "Traning: 16500; Loss: 0.45917901396751404\n",
      "total test loss: 20.800628393888474\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 16600; Loss: 0.4580990970134735\n",
      "Traning: 16700; Loss: 0.3796335458755493\n",
      "Traning: 16800; Loss: 0.5551097989082336\n",
      "total test loss: 20.789917171001434\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 16900; Loss: 0.5771117806434631\n",
      "Traning: 17000; Loss: 0.3674522340297699\n",
      "Traning: 17100; Loss: 0.4578445255756378\n",
      "Traning: 17200; Loss: 0.3169342279434204\n",
      "total test loss: 20.755262911319733\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 17300; Loss: 0.3793536126613617\n",
      "Traning: 17400; Loss: 0.3955844044685364\n",
      "Traning: 17500; Loss: 0.41883984208106995\n",
      "Traning: 17600; Loss: 0.5157952904701233\n",
      "total test loss: 20.75791370868683\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 17700; Loss: 0.5315728187561035\n",
      "Traning: 17800; Loss: 0.3354102373123169\n",
      "Traning: 17900; Loss: 0.49792760610580444\n",
      "Traning: 18000; Loss: 0.3786633312702179\n",
      "total test loss: 20.738835871219635\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 18100; Loss: 0.46371403336524963\n",
      "Traning: 18200; Loss: 0.45243746042251587\n",
      "Traning: 18300; Loss: 0.49082356691360474\n",
      "total test loss: 20.733112007379532\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 18400; Loss: 0.586013674736023\n",
      "Traning: 18500; Loss: 0.38570353388786316\n",
      "Traning: 18600; Loss: 0.4180850684642792\n",
      "Traning: 18700; Loss: 0.425765722990036\n",
      "total test loss: 20.736844211816788\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 18800; Loss: 0.380138635635376\n",
      "Traning: 18900; Loss: 0.31132012605667114\n",
      "Traning: 19000; Loss: 0.41825467348098755\n",
      "Traning: 19100; Loss: 0.4893507957458496\n",
      "total test loss: 20.730228662490845\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 19200; Loss: 0.5258134007453918\n",
      "Traning: 19300; Loss: 0.4888034760951996\n",
      "Traning: 19400; Loss: 0.4199316203594208\n",
      "Traning: 19500; Loss: 0.4093855917453766\n",
      "total test loss: 20.730006724596024\n",
      "total accuracy: 0.8136666417121887\n",
      "Traning: 19600; Loss: 0.4385013282299042\n",
      "Traning: 19700; Loss: 0.4018653631210327\n",
      "Traning: 19800; Loss: 0.3206363916397095\n",
      "total test loss: 20.708914875984192\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 19900; Loss: 0.35843029618263245\n",
      "Traning: 20000; Loss: 0.4097104072570801\n",
      "Traning: 20100; Loss: 0.3612445592880249\n",
      "Traning: 20200; Loss: 0.48024219274520874\n",
      "total test loss: 20.729397773742676\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 20300; Loss: 0.40641969442367554\n",
      "Traning: 20400; Loss: 0.42039698362350464\n",
      "Traning: 20500; Loss: 0.41537153720855713\n",
      "Traning: 20600; Loss: 0.4037841856479645\n",
      "total test loss: 20.715937465429306\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 20700; Loss: 0.46053436398506165\n",
      "Traning: 20800; Loss: 0.48074090480804443\n",
      "Traning: 20900; Loss: 0.3529389798641205\n",
      "Traning: 21000; Loss: 0.5165372490882874\n",
      "total test loss: 20.71825297176838\n",
      "total accuracy: 0.8136666417121887\n",
      "Traning: 21100; Loss: 0.4551923871040344\n",
      "Traning: 21200; Loss: 0.4265287518501282\n",
      "Traning: 21300; Loss: 0.4361678659915924\n",
      "total test loss: 20.71231970191002\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 21400; Loss: 0.4863000810146332\n",
      "Traning: 21500; Loss: 0.421919584274292\n",
      "Traning: 21600; Loss: 0.4441315531730652\n",
      "Traning: 21700; Loss: 0.401642382144928\n",
      "total test loss: 20.714481472969055\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 21800; Loss: 0.3526296317577362\n",
      "Traning: 21900; Loss: 0.4827091693878174\n",
      "Traning: 22000; Loss: 0.5110220909118652\n",
      "Traning: 22100; Loss: 0.46630144119262695\n",
      "total test loss: 20.723273172974586\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 22200; Loss: 0.3525364100933075\n",
      "Traning: 22300; Loss: 0.38519683480262756\n",
      "Traning: 22400; Loss: 0.5924824476242065\n",
      "Traning: 22500; Loss: 0.543940007686615\n",
      "total test loss: 20.70713710784912\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 22600; Loss: 0.4202745854854584\n",
      "Traning: 22700; Loss: 0.4135584533214569\n",
      "Traning: 22800; Loss: 0.3479083180427551\n",
      "total test loss: 20.71145921945572\n",
      "total accuracy: 0.8133333325386047\n",
      "Traning: 22900; Loss: 0.42410585284233093\n",
      "Traning: 23000; Loss: 0.4063047170639038\n",
      "Traning: 23100; Loss: 0.41741636395454407\n",
      "Traning: 23200; Loss: 0.4166628122329712\n",
      "total test loss: 20.694037228822708\n",
      "total accuracy: 0.812666654586792\n",
      "Traning: 23300; Loss: 0.3992427587509155\n",
      "Traning: 23400; Loss: 0.44765356183052063\n",
      "Traning: 23500; Loss: 0.39706963300704956\n",
      "Traning: 23600; Loss: 0.37728554010391235\n",
      "total test loss: 20.689345628023148\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 23700; Loss: 0.40540000796318054\n",
      "Traning: 23800; Loss: 0.44511479139328003\n",
      "Traning: 23900; Loss: 0.3911321461200714\n",
      "Traning: 24000; Loss: 0.4716981053352356\n",
      "total test loss: 20.695676535367966\n",
      "total accuracy: 0.8136666417121887\n",
      "Traning: 24100; Loss: 0.4567975699901581\n",
      "Traning: 24200; Loss: 0.37833070755004883\n",
      "Traning: 24300; Loss: 0.5597041845321655\n",
      "total test loss: 20.67478483915329\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 24400; Loss: 0.4111410975456238\n",
      "Traning: 24500; Loss: 0.5784788727760315\n",
      "Traning: 24600; Loss: 0.41875895857810974\n",
      "Traning: 24700; Loss: 0.5341520309448242\n",
      "total test loss: 20.681820154190063\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 24800; Loss: 0.4852333664894104\n",
      "Traning: 24900; Loss: 0.48418837785720825\n",
      "Traning: 25000; Loss: 0.5463276505470276\n",
      "Traning: 25100; Loss: 0.40519779920578003\n",
      "total test loss: 20.677287876605988\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 25200; Loss: 0.4229496121406555\n",
      "Traning: 25300; Loss: 0.5370470285415649\n",
      "Traning: 25400; Loss: 0.5315080285072327\n",
      "Traning: 25500; Loss: 0.433327317237854\n",
      "total test loss: 20.67239898443222\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 25600; Loss: 0.4181686043739319\n",
      "Traning: 25700; Loss: 0.4172190725803375\n",
      "Traning: 25800; Loss: 0.42955395579338074\n",
      "total test loss: 20.66534063220024\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 25900; Loss: 0.3910655677318573\n",
      "Traning: 26000; Loss: 0.4556976854801178\n",
      "Traning: 26100; Loss: 0.4287208914756775\n",
      "Traning: 26200; Loss: 0.5180299282073975\n",
      "total test loss: 20.65979152917862\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 26300; Loss: 0.37504181265830994\n",
      "Traning: 26400; Loss: 0.4581589102745056\n",
      "Traning: 26500; Loss: 0.49638813734054565\n",
      "Traning: 26600; Loss: 0.3539187014102936\n",
      "total test loss: 20.668938130140305\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 26700; Loss: 0.419210821390152\n",
      "Traning: 26800; Loss: 0.3802244961261749\n",
      "Traning: 26900; Loss: 0.41463252902030945\n",
      "Traning: 27000; Loss: 0.6533865332603455\n",
      "total test loss: 20.66337651014328\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 27100; Loss: 0.486111044883728\n",
      "Traning: 27200; Loss: 0.4465161859989166\n",
      "Traning: 27300; Loss: 0.42952054738998413\n",
      "total test loss: 20.678528010845184\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 27400; Loss: 0.5005914568901062\n",
      "Traning: 27500; Loss: 0.5216447114944458\n",
      "Traning: 27600; Loss: 0.4502235949039459\n",
      "Traning: 27700; Loss: 0.4689226448535919\n",
      "total test loss: 20.65279245376587\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 27800; Loss: 0.35839182138442993\n",
      "Traning: 27900; Loss: 0.5057529807090759\n",
      "Traning: 28000; Loss: 0.6359879970550537\n",
      "Traning: 28100; Loss: 0.4604462683200836\n",
      "total test loss: 20.636944502592087\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 28200; Loss: 0.35607340931892395\n",
      "Traning: 28300; Loss: 0.45770570635795593\n",
      "Traning: 28400; Loss: 0.4735730290412903\n",
      "Traning: 28500; Loss: 0.4227908253669739\n",
      "total test loss: 20.66880989074707\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 28600; Loss: 0.4207235872745514\n",
      "Traning: 28700; Loss: 0.5124341249465942\n",
      "Traning: 28800; Loss: 0.3221764862537384\n",
      "total test loss: 20.65786448121071\n",
      "total accuracy: 0.8140000104904175\n",
      "Traning: 28900; Loss: 0.37673595547676086\n",
      "Traning: 29000; Loss: 0.3971642553806305\n",
      "Traning: 29100; Loss: 0.3145931363105774\n",
      "Traning: 29200; Loss: 0.3587821125984192\n",
      "total test loss: 20.658495515584946\n",
      "total accuracy: 0.8133333325386047\n",
      "Traning: 29300; Loss: 0.528608500957489\n",
      "Traning: 29400; Loss: 0.5541645884513855\n",
      "Traning: 29500; Loss: 0.5176103711128235\n",
      "Traning: 29600; Loss: 0.5067821741104126\n",
      "total test loss: 20.66826683282852\n",
      "total accuracy: 0.8143333196640015\n",
      "Traning: 29700; Loss: 0.40925702452659607\n",
      "Traning: 29800; Loss: 0.2790609896183014\n",
      "Traning: 29900; Loss: 0.3928464949131012\n",
      "Traning: 30000; Loss: 0.35995355248451233\n",
      "total test loss: 20.657526522874832\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 30100; Loss: 0.4605509638786316\n",
      "Traning: 30200; Loss: 0.3719714283943176\n",
      "Traning: 30300; Loss: 0.5282727479934692\n",
      "total test loss: 20.64635819196701\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 30400; Loss: 0.4554705321788788\n",
      "Traning: 30500; Loss: 0.3369024991989136\n",
      "Traning: 30600; Loss: 0.6131947636604309\n",
      "Traning: 30700; Loss: 0.4522019922733307\n",
      "total test loss: 20.65679594874382\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 30800; Loss: 0.47827914357185364\n",
      "Traning: 30900; Loss: 0.4500468373298645\n",
      "Traning: 31000; Loss: 0.7080112099647522\n",
      "Traning: 31100; Loss: 0.45788827538490295\n",
      "total test loss: 20.681052207946777\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 31200; Loss: 0.35029464960098267\n",
      "Traning: 31300; Loss: 0.34187185764312744\n",
      "Traning: 31400; Loss: 0.3727259337902069\n",
      "Traning: 31500; Loss: 0.5454367995262146\n",
      "total test loss: 20.648639053106308\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 31600; Loss: 0.5390146970748901\n",
      "Traning: 31700; Loss: 0.45864784717559814\n",
      "Traning: 31800; Loss: 0.37544065713882446\n",
      "total test loss: 20.668240040540695\n",
      "total accuracy: 0.8146666884422302\n",
      "Traning: 31900; Loss: 0.4186488389968872\n",
      "Traning: 32000; Loss: 0.4829876720905304\n",
      "Traning: 32100; Loss: 0.46520766615867615\n",
      "Traning: 32200; Loss: 0.4443182945251465\n",
      "total test loss: 20.641239494085312\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 32300; Loss: 0.36396080255508423\n",
      "Traning: 32400; Loss: 0.3826214671134949\n",
      "Traning: 32500; Loss: 0.4423319399356842\n",
      "Traning: 32600; Loss: 0.4473329484462738\n",
      "total test loss: 20.65152509510517\n",
      "total accuracy: 0.815666675567627\n",
      "Traning: 32700; Loss: 0.36339104175567627\n",
      "Traning: 32800; Loss: 0.4002630114555359\n",
      "Traning: 32900; Loss: 0.46344059705734253\n",
      "Traning: 33000; Loss: 0.3765733242034912\n",
      "total test loss: 20.655237913131714\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 33100; Loss: 0.42006054520606995\n",
      "Traning: 33200; Loss: 0.3790191113948822\n",
      "Traning: 33300; Loss: 0.4097500443458557\n",
      "total test loss: 20.655273377895355\n",
      "total accuracy: 0.815666675567627\n",
      "Traning: 33400; Loss: 0.49435147643089294\n",
      "Traning: 33500; Loss: 0.5291719436645508\n",
      "Traning: 33600; Loss: 0.5814799666404724\n",
      "Traning: 33700; Loss: 0.4149821400642395\n",
      "total test loss: 20.65748107433319\n",
      "total accuracy: 0.8159999847412109\n",
      "Traning: 33800; Loss: 0.353273868560791\n",
      "Traning: 33900; Loss: 0.3888988494873047\n",
      "Traning: 34000; Loss: 0.4001198709011078\n",
      "Traning: 34100; Loss: 0.46312856674194336\n",
      "total test loss: 20.655174791812897\n",
      "total accuracy: 0.815666675567627\n",
      "Traning: 34200; Loss: 0.30214977264404297\n",
      "Traning: 34300; Loss: 0.34219661355018616\n",
      "Traning: 34400; Loss: 0.48359617590904236\n",
      "Traning: 34500; Loss: 0.4372389316558838\n",
      "total test loss: 20.657704442739487\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 34600; Loss: 0.37543848156929016\n",
      "Traning: 34700; Loss: 0.3733643591403961\n",
      "Traning: 34800; Loss: 0.4572742283344269\n",
      "total test loss: 20.665403336286545\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 34900; Loss: 0.38322970271110535\n",
      "Traning: 35000; Loss: 0.4120088219642639\n",
      "Traning: 35100; Loss: 0.4866199791431427\n",
      "Traning: 35200; Loss: 0.3026341497898102\n",
      "total test loss: 20.64811933040619\n",
      "total accuracy: 0.8153333067893982\n",
      "Traning: 35300; Loss: 0.37953832745552063\n",
      "Traning: 35400; Loss: 0.36156898736953735\n",
      "Traning: 35500; Loss: 0.6061037182807922\n",
      "Traning: 35600; Loss: 0.41779616475105286\n",
      "total test loss: 20.65383145213127\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 35700; Loss: 0.3909507691860199\n",
      "Traning: 35800; Loss: 0.3491850197315216\n",
      "Traning: 35900; Loss: 0.38119134306907654\n",
      "Traning: 36000; Loss: 0.39786240458488464\n",
      "total test loss: 20.64156737923622\n",
      "total accuracy: 0.8163333535194397\n",
      "Traning: 36100; Loss: 0.39042776823043823\n",
      "Traning: 36200; Loss: 0.4107133448123932\n",
      "Traning: 36300; Loss: 0.3481529653072357\n",
      "total test loss: 20.64753270149231\n",
      "total accuracy: 0.815666675567627\n",
      "Traning: 36400; Loss: 0.47532132267951965\n",
      "Traning: 36500; Loss: 0.35002440214157104\n",
      "Traning: 36600; Loss: 0.3998132646083832\n",
      "Traning: 36700; Loss: 0.3307197093963623\n",
      "total test loss: 20.64364105463028\n",
      "total accuracy: 0.8149999976158142\n",
      "Traning: 36800; Loss: 0.5413439273834229\n",
      "Traning: 36900; Loss: 0.4899052679538727\n",
      "Traning: 37000; Loss: 0.37383174896240234\n",
      "Traning: 37100; Loss: 0.472906231880188\n",
      "total test loss: 20.662762820720673\n",
      "total accuracy: 0.8163333535194397\n",
      "------------epoch: 100------------\n",
      "Traning: 37200; Loss: 0.34102240204811096\n",
      "Traning: 37300; Loss: 0.41351473331451416\n",
      "Traning: 37400; Loss: 0.39036184549331665\n",
      "Traning: 37500; Loss: 0.39405280351638794\n",
      "total test loss: 20.652111560106277\n",
      "total accuracy: 0.815666675567627\n"
     ]
    }
   ],
   "source": [
    "training_nn(epochs=100, learning_rate=0.01, loss_func='CE', optimizer_para='Adam',\n",
    "                input_feature=23, output_feature=2, dim1=36, dim2=108, dropout=0.8, val_size = 3000,\n",
    "                train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
